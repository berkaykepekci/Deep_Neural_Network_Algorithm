{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    sig= 1 / ( 1 + np.exp(-Z))\n",
    "    \n",
    "    return sig , Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    return A, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z)+np.exp(-Z))\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    parameters={}\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    for l in range(1,len(layer_dims)):\n",
    "        parameters[\"W\"+str(l)]= np.random.randn(layer_dims[l],layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters[\"b\"+str(l)]= np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    \n",
    "    Z=np.dot(W , A)+ b\n",
    "    \n",
    "    cache= (A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_act_forward(A_prev, W, b, act_function=\"sigmoid\"):\n",
    "    \n",
    "    if act_function==\"sigmoid\":\n",
    "        Z , linear_cache = linear_forward(A_prev,W,b)\n",
    "        A , activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif act_function==\"relu\":\n",
    "        Z , linear_cache= linear_forward(A_prev,W,b)\n",
    "        A , activation_cache = relu(Z)\n",
    "        \n",
    "    elif act_function==\"tanh\":\n",
    "        Z , linear_cache = linear_forward(A_prev,W,b)\n",
    "        A , activation_cache = tanh(Z)\n",
    "        \n",
    "    cache = (linear_cache,activation_cache)\n",
    "    \n",
    "    return A , cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, parameters,act_pref=\"relu\"):\n",
    "    \n",
    "    caches=[]\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b= parameters [\"b\"+str(l)]\n",
    "        A , cache = linear_act_forward(A_prev,W,b, act_function=act_pref)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    W=parameters[\"W\"+str(L)]\n",
    "    b=parameters[\"b\"+str(L)]\n",
    "    AL , cache = linear_act_forward(A,W,b,act_function=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m=Y.shape[1]\n",
    "    \n",
    "    cost=-np.sum((Y*np.log(AL)+(1-Y)*np.log(1-AL))) / m \n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    m=dZ.shape[1]\n",
    "    \n",
    "    A_prev=cache[0]\n",
    "    W=cache[1]\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T)*(1/m)\n",
    "    db = np.sum(dZ,axis=1,keepdims=True) * (1/m)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = dA * (1-(tanh(Z)[0]**2))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_act_backward(dA, cache, act_function=\"sigmoid\"):\n",
    "    \n",
    "    linear_cache,activation_cache = cache\n",
    "    \n",
    "    if act_function==\"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dW, db, dA_prev = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif act_function==\"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dW, db, dA_prev = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif act_function==\"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        dW, db, dA_prev = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_model(AL,Y,caches,act_pref=\"relu\"):\n",
    "    \n",
    "    grads=dict()\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    \n",
    "    L = len(caches)\n",
    "    \n",
    "    grads[\"dW\"+str(L)],grads[\"db\"+str(L)],grads[\"dA\"+str(L-1)]=linear_act_backward(dAL,caches[L-1],act_function=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(1,L)):\n",
    "        current_cache=caches[l-1]\n",
    "        dW_temp,db_temp,dA_temp=linear_act_backward(grads[\"dA\"+str(l)],current_cache,act_function=act_pref)\n",
    "        grads[\"dW\"+str(l)]=dW_temp\n",
    "        grads[\"db\"+str(l)]=db_temp\n",
    "        grads[\"dA\"+str(l-1)]=dA_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \n",
    "    L=len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]=parameters[\"W\"+str(l+1)] - learning_rate * (grads[\"dW\"+str(l+1)])\n",
    "        parameters[\"b\"+str(l+1)]=parameters[\"b\"+str(l+1)] - learning_rate * (grads[\"db\" + str(l+1)])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(X,\n",
    "              Y, \n",
    "              layer_dims,\n",
    "              learning_rate=0.0075,\n",
    "              iteration=2500,\n",
    "              print_cost=False,\n",
    "              act_pref=\"relu\"):\n",
    "    \n",
    "    costs=[]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(0,iteration):\n",
    "        \n",
    "        AL, caches = linear_model(X,parameters,act_pref)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = backward_model(AL, Y, caches,act_pref)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "   \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(X,\n",
    "              Y,\n",
    "              layer_dims,\n",
    "              learning_rate=0.0075,\n",
    "              iteration=2500,\n",
    "              print_cost=False,\n",
    "              act_pref=\"relu\",\n",
    "              early_stopping=False,\n",
    "              eval_set=[]):\n",
    "    \n",
    "    costs=[]\n",
    "    accuracy_scores=[]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    if early_stopping==False:\n",
    "        \n",
    "        for i in range(0,iteration):\n",
    "\n",
    "            AL, caches = linear_model(X,parameters,act_pref)\n",
    "\n",
    "            cost = compute_cost(AL, Y)\n",
    "\n",
    "            grads = backward_model(AL, Y, caches,act_pref)\n",
    "\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Train cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "    \n",
    "    \n",
    "    if early_stopping==True:\n",
    "        \n",
    "        x_eval=eval_set[0]\n",
    "        y_eval=eval_set[1]\n",
    "        \n",
    "        for i in range(0,iteration):\n",
    "\n",
    "            AL, caches = linear_model(X,parameters,act_pref)\n",
    "\n",
    "            cost = compute_cost(AL, Y)\n",
    "\n",
    "            grads = backward_model(AL, Y, caches,act_pref)\n",
    "\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Train cost after iteration %i: %f\" %(i, cost),end=\"   \")\n",
    "                \n",
    "                \n",
    "                y_pred_test=predict(test_x,parameters)\n",
    "                accuracy, trues = accuracy_score_me(y_pred_test,test_y)\n",
    "                \n",
    "                print(\"Test set accuracy: %f\" %(accuracy))\n",
    "                \n",
    "                \n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "                accuracy_scores.append(accuracy)\n",
    "                \n",
    "    \n",
    "          \n",
    "    if print_cost==True:\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.squeeze(accuracy_scores))\n",
    "        plt.ylabel('accuracy score for test set')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    y_pred=np.zeros((1,m))\n",
    "    \n",
    "    probs, caches = linear_model(X,parameters)\n",
    "    \n",
    "    for i in range(len(probs[0])):\n",
    "        \n",
    "        if probs[0][i] >= 0.5:\n",
    "            y_pred[0][i]=1\n",
    "        \n",
    "        else:\n",
    "            y_pred[0][i]=0\n",
    "    \n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_my(y_pred,y_true):\n",
    "    \n",
    "    m=y_true.shape[1]\n",
    "    \n",
    "    acc=np.sum((y_pred==y_true)) / m\n",
    "    trues=np.sum((y_pred==y_true))\n",
    "    \n",
    "    \n",
    "    return acc ,trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred,y_true):\n",
    "    \n",
    "    true_p=np.sum((y_pred==y_true),where=(y_pred==1))\n",
    "    true_n=np.sum((y_pred==y_true),where=(y_pred==0))\n",
    "    false_p=np.sum((y_pred!=y_true),where=(y_pred==1))\n",
    "    false_n=np.sum((y_pred!=y_true),where=(y_pred==0))\n",
    "    \n",
    "    matrix=np.array([true_n,false_p,false_n,true_p]).reshape(2,2)\n",
    "    df=pd.DataFrame(matrix,index=[0,1],columns=[0,1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(pred_y, true_y, title='Confusion Matrix', figsize=(8,6)):\n",
    "    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n",
    "    conf_matrix = confusion_matrix(pred_y, true_y)\n",
    "    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n",
    "    conf_df.index.name = 'Actual'\n",
    "    conf_df.columns.name = 'Predicted'\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.title(title)\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n",
    "                annot_kws={\"size\": 16}, fmt='g')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function needs the model algorithm to be edited. dnn_model must return \"max_acc\"\n",
    "\n",
    "def learning_rate_search(learning_rate_grid=[0.0075]):\n",
    "    max_acc_list=[]\n",
    "    for lr in learning_rate_grid:\n",
    "        params,max_acc=dnn_model(train_x, train_y, layers_dims, learning_rate=lr, iteration=500,print_cost=False,early_stopping=True,eval_set=[test_x,test_y])\n",
    "        max_acc_list.append(max_acc)\n",
    "    \n",
    "    max_of_maxes=np.max(max_acc_list)\n",
    "    ix=max_acc_list.index(max_of_maxes)\n",
    "    \n",
    "    return max_of_maxes, learning_rate_grid[ix] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
